{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e66c1e89-3936-4608-b2ca-4f32bde575a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Union, Tuple, Optional, Callable\n",
    "\n",
    "from backbones.vit import (\n",
    "    ViT,\n",
    "    Transformer,\n",
    "    vit_nano,\n",
    "    vit_micro,\n",
    "    vit_tiny,\n",
    "    vit_small,\n",
    "    vit_base,\n",
    "    vit_large,\n",
    ")\n",
    "\n",
    "\n",
    "class MAE(nn.Module):\n",
    "    \"\"\"Masked autoencoder with a vision transformer encoder and decoder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    backbone : nn.Module\n",
    "        Vision transformer backbone\n",
    "    img_size : Union[int, Tuple[int, int]]\n",
    "        Size of the input image\n",
    "    patch_size : Union[int, Tuple[int, int]]\n",
    "        Size of the masked patches\n",
    "    in_chans : int\n",
    "        Number of input channels\n",
    "    mask_ratio : float, optional\n",
    "        Ratio of masked patches\n",
    "    embed_dim : int, optional\n",
    "        Dimension of the embedding\n",
    "    depth : int, optional\n",
    "        The number of transformer blocks\n",
    "    num_heads : int, optional\n",
    "        Number of attention heads\n",
    "    mlp_ratio : float, optional\n",
    "        Ratio of the hidden dimension to the embedding dimension\n",
    "    pre_norm : bool, optional\n",
    "        Whether to apply layer normalization before the attention layer\n",
    "    decoder_embed_dim : int, optional\n",
    "        Dimension of the decoder embedding\n",
    "    decoder_depth : int, optional\n",
    "        Number of decoder transformer blocks\n",
    "    decoder_num_heads : int, optional\n",
    "        Number of decoder attention heads\n",
    "    norm_layer : Optional[Callable], optional\n",
    "        Normalization layer\n",
    "    patch_norm_layer : Optional[Callable], optional\n",
    "        Patch normalization layer\n",
    "    post_norm_layer : Optional[Callable]\n",
    "        The post-normalization layer\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    1. K. He, X. Chen, S. Xie, Y. Li, P. DollÃ¡r, R. Girshick, \"Masked\n",
    "       Autoencoders Are Scalable Vision Learners\". CVPR 2022.\n",
    "       https://arxiv.org/abs/2111.06377.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        mask_ratio: float = 0.5,\n",
    "        decoder_dim: int = 768,\n",
    "        decoder_depth: int = 12,\n",
    "        decoder_heads: int = 12,\n",
    "        decoder_dim_head: int = 64,\n",
    "    ):\n",
    "        super(MAE, self).__init__()\n",
    "        self.arguments = locals()\n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "        self.encoder = backbone\n",
    "\n",
    "        self.to_patch_embedding = backbone.to_patch_embedding\n",
    "\n",
    "        self.patch_height = backbone.patch_height\n",
    "        self.patch_width = backbone.patch_width\n",
    "        self.n_registers = backbone.n_registers\n",
    "        self.in_chans = backbone.in_chans\n",
    "        \n",
    "        embed_dim = backbone.dim\n",
    "        n_patches = backbone.n_patches        \n",
    "        n_patch_pixels = self.patch_height * self.patch_width * self.in_chans\n",
    "\n",
    "        self.to_decoder = nn.Identity()\n",
    "        if embed_dim != decoder_dim:\n",
    "            self.to_decoder = nn.Linear(embed_dim, decoder_dim)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.randn(decoder_dim))\n",
    "\n",
    "        self.decoder = Transformer(\n",
    "            dim=decoder_dim,\n",
    "            depth=decoder_depth,\n",
    "            heads=decoder_heads,\n",
    "            dim_head=decoder_dim_head,\n",
    "            mlp_dim=decoder_dim * 4\n",
    "        )\n",
    "\n",
    "        self.decoder_pos_embed = nn.Embedding(n_patches, decoder_dim)\n",
    "        self.decoder_norm = nn.LayerNorm(decoder_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_dim,  n_patch_pixels, bias=True)\n",
    "\n",
    "    def _patches_to_img(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.img_size[0] // self.patch_height\n",
    "        w = self.img_size[1] // self.patch_width\n",
    "        ph = self.patch_height\n",
    "        pw = self.patch_width\n",
    "        c = self.in_chans\n",
    "        \n",
    "        x = x.view(-1, h, w, ph, pw, c)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "        x = x.reshape(1, c, h * ph, w * pw)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def random_masking(self, tokens: torch.Tensor, mask_ratio: float):\n",
    "        device = tokens.device\n",
    "        b, n_patches, *_ = tokens.shape\n",
    "        print(\"N_PATCHES\", n_patches)\n",
    "\n",
    "        n_masked = int((1 - mask_ratio) * n_patches)\n",
    "        idx = torch.rand(b, n_patches, device=device).argsort(dim=-1)\n",
    "        mask, unmask = idx[:, :n_masked], idx[:, n_masked:]\n",
    "\n",
    "        batch_range = torch.arange(b)[:, None]\n",
    "        tokens = tokens[batch_range, unmask]\n",
    "\n",
    "        return tokens, mask, unmask\n",
    "\n",
    "    def forward_encoder(self, x: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        b, c, h, w = x.shape\n",
    "        p1, p2 = self.patch_height, self.patch_width\n",
    "\n",
    "        ph = h // p1\n",
    "        pw = w // p2\n",
    "\n",
    "        x = x.reshape(b, c, ph, p1, pw, p2)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)\n",
    "        x = x.reshape(b, ph * pw, p1 * p2 * c)\n",
    "\n",
    "        tokens = self.to_patch_embedding(x)\n",
    "\n",
    "        cls_tokens = self.encoder.cls_token.expand(b, -1, -1)\n",
    "        tokens = torch.cat((cls_tokens, tokens), dim=1)\n",
    "\n",
    "        tokens += self.encoder.pos_embedding[:, :(ph * pw + 1)]\n",
    "        tokens = self.encoder.dropout(tokens)\n",
    "\n",
    "        r = self.encoder.register_tokens.expand(b, -1, -1)\n",
    "\n",
    "        tokens = torch.cat((r, tokens), dim=1)\n",
    "        tokens = self.encoder.transformer(tokens)\n",
    "        tokens = tokens[:, self.encoder.n_registers + 1:, :]\n",
    "\n",
    "        tokens, mask, unmask = self.random_masking(tokens, self.mask_ratio)\n",
    "\n",
    "        return x, tokens, mask, unmask\n",
    "\n",
    "    def forward_decoder(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        unmask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        device = tokens.device\n",
    "        b = tokens.shape[0]\n",
    "        n_masked, n_unmasked = mask.shape[1], unmask.shape[1]\n",
    "\n",
    "        tokens = self.to_decoder(tokens)\n",
    "        unmasked_tokens = tokens + self.decoder_pos_embed(unmask)\n",
    "\n",
    "        mask_tokens = self.mask_token.repeat(b, n_masked, 1)\n",
    "        mask_tokens = mask_tokens + self.decoder_pos_embed(mask)\n",
    "\n",
    "        decoder_tokens = torch.zeros(\n",
    "            b,\n",
    "            n_masked + n_unmasked,\n",
    "            self.decoder_dim,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        batch_range = torch.arange(b)[:, None]\n",
    "\n",
    "        decoder_tokens[batch_range, unmask] = unmasked_tokens\n",
    "        decoder_tokens[batch_range, mask] = mask_tokens\n",
    "\n",
    "        decoded_tokens = self.decoder(decoder_tokens)\n",
    "        decoded_tokens = self.decoder_pred(decoded_tokens)\n",
    "\n",
    "        return decoded_tokens\n",
    "\n",
    "    def forward_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        decoded_tokens: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        loss = F.mse_loss(decoded_tokens, x, reduction='mean')\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x: torch.Tensor, with_reconstructed: bool = False) -> torch.Tensor:\n",
    "        device = x.device\n",
    "        b, c, h, w = x.shape\n",
    "        self.img_size = (h, w)\n",
    "        \n",
    "        x, tokens, mask, unmask = self.forward_encoder(x)\n",
    "\n",
    "        decoded_tokens = self.forward_decoder(tokens, mask, unmask)\n",
    "\n",
    "        loss = self.forward_loss(x, decoded_tokens)\n",
    "\n",
    "        if with_reconstructed:\n",
    "            batch_range = torch.arange(b)[:, None]\n",
    "            reconstructed = torch.zeros(decoded_tokens.shape, device=device)\n",
    "            reconstructed[batch_range, unmask] = x[batch_range, unmask]\n",
    "            reconstructed[batch_range, mask] = decoded_tokens[batch_range, mask]\n",
    "            return loss, self._patches_to_img(reconstructed)\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfdf28ab-663a-4363-ae3f-b9588c8a3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_PATCHES 196\n",
      "tensor(1.3374, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MAE(vit_micro())\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "add7f1fa-6955-41db-8a7d-91e5803c3e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "x = Image.open(\"/Users/tomouellette/Home/data/pollen-data/cells-livecell/LIVECELL00083_image.png\")\n",
    "x = x.resize((224, 224))\n",
    "x = torch.from_numpy(np.array(x))[None, None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d9fa721-f519-491d-abbc-5188e01fdcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7e26c82-797b-41af-8c16-38aa3465cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = MAE(vit_micro(channels=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9c76b47-10d8-47fc-ae95-beb22a3ec9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_PATCHES 196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16410.1250, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b mae(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8de10b8c-c376-4fb7-9dbd-5093ee527245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_PATCHES 196\n"
     ]
    }
   ],
   "source": [
    "loss, reconstructed = model(x, with_reconstructed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e3bcc-782b-4de2-8757-fdce0d6ae461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87edf12e-28b4-4165-bfea-f6941088a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "x = torch.rand(1, 196, 768)\n",
    "pixels = rearrange(\n",
    "    x,\n",
    "    'b (h w) (ph pw c) -> b c (h ph) (w pw)',\n",
    "    h = model.img_size[0] // model.patch_height,\n",
    "    w = model.img_size[1] // model.patch_width,\n",
    "    ph = model.patch_height, pw = model.patch_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f99c16c-a391-4ef0-9375-dcad96afd0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input\n",
    "# x = torch.rand(1, 196, 768)\n",
    "\n",
    "# Parameters\n",
    "h = model.img_size[0] // model.patch_height\n",
    "w = model.img_size[1] // model.patch_width\n",
    "ph = model.patch_height\n",
    "pw = model.patch_width\n",
    "c = 768 // (ph * pw)\n",
    "\n",
    "# Step 1: Reshape x to separate the pixel components\n",
    "x = x.view(1, h, w, ph, pw, c)\n",
    "\n",
    "# Step 2: Permute the dimensions to move the channels to the second position\n",
    "x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "\n",
    "# Step 3: Reshape to combine height and patch height, and width and patch width\n",
    "pixels2 = x.reshape(1, c, h * ph, w * pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30e108d5-60e0-4278-b819-5aca0d4ccc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels == pixels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a9b0183-f118-4466-9a97-97bdf62253e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x # .permute(0, 2, 1)\n",
    "y = y.reshape(-1, 3, model.img_size[0], model.img_size[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32c39a40-99e4-495a-8ef3-0d6cec554e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False,  True]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y == pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f68c2740-ec02-4257-a4d5-64e497a93eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a97fa6a9-7b5c-47d2-b53a-563e09e3ef96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a40b177-9f36-4be0-aa56-a481992ad630",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mreconstructed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(reconstructed[0][0].detach().permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ee7ff-d1e0-4235-87a8-f9e6a0638ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patches_to_img(self, decoded_tokens: torch.Tensor) -> torch.Tensor:\n",
    "    decoded_tokens = decoded_tokens.permute(0, 2, 1)\n",
    "    decoded_tokens = decoded_tokens.reshape(\n",
    "        decoded_tokens.shape[0], decoded_tokens.shape[1], self.patch_height, self.patch_width\n",
    "    )\n",
    "    decoded_tokens = decoded_tokens.permute(0, 1, 2, 3)\n",
    "    \n",
    "    pixels = rearrange(\n",
    "        decoded_tokens,\n",
    "        'b (h w) (ph pw c) -> b c (h ph) (w pw)',\n",
    "        h = self.img_size[0] // self.patch_height,\n",
    "        w = self.img_size[1] // self.patch_width,\n",
    "        ph = self.patch_height, pw = self.patch_width,\n",
    "    )\n",
    "    \n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70b8bf-c3e1-484c-bb50-85578e61dcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
